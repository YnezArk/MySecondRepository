import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import re
from datetime import datetime, timedelta
import time
import os


from datetime import datetime


def get_sicau_notices():
    """
    è·å–å››å·å†œä¸šå¤§å­¦ç”Ÿå‘½ç§‘å­¦å­¦é™¢é€šçŸ¥å…¬å‘Š
    """
    # ç”Ÿå‘½ç§‘å­¦å­¦é™¢é€šçŸ¥å…¬å‘Šé¡µé¢URL
    base_url = "https://smkx.sicau.edu.cn/xwjtz/tzgg.htm"

    all_notices = []

    try:
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://life.sicau.edu.cn/'
        }

        print(f"æ­£åœ¨è®¿é—®å››å·å†œä¸šå¤§å­¦ç”Ÿå‘½ç§‘å­¦å­¦é™¢é€šçŸ¥å…¬å‘Šé¡µé¢...")
        print(f"URL: {base_url}")

        # å‘é€GETè¯·æ±‚
        response = requests.get(base_url, headers=headers, timeout=15)
        response.encoding = 'utf-8'  # ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º

        if response.status_code != 200:
            print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return all_notices

        print("é¡µé¢è®¿é—®æˆåŠŸï¼Œå¼€å§‹è§£æå†…å®¹...")

        # è§£æHTMLå†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')

        # æ–¹æ³•1ï¼šå°è¯•æŸ¥æ‰¾é€šçŸ¥åˆ—è¡¨å®¹å™¨ï¼ˆåŸºäºå¸¸è§ç»“æ„ï¼‰
        notice_list = soup.find('ul', class_=re.compile(r'sub-list|news-list|list-group'))

        # æ–¹æ³•2ï¼šå¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«é€šçŸ¥æ ‡é¢˜çš„æ‰€æœ‰é“¾æ¥
        if not notice_list:
            print("æœªæ‰¾åˆ°æ ‡å‡†åˆ—è¡¨å®¹å™¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•è§£æ...")
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ—¥æœŸçš„å…ƒç´ 
            notice_items = soup.find_all(text=re.compile(r'202[0-9]-[0-9]{2}-[0-9]{2}'))
            notice_list = soup  # åœ¨æ•´ä¸ªé¡µé¢ä¸­æœç´¢

        else:
            # åœ¨åˆ—è¡¨å®¹å™¨ä¸­æŸ¥æ‰¾æ‰€æœ‰é€šçŸ¥é¡¹
            notice_items = notice_list.find_all('li') or notice_list.find_all('div', class_=re.compile(r'news|notice'))

        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„æ–¹æ³•
        if not notice_items:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é“¾æ¥å’Œæ—¥æœŸçš„å…ƒç´ 
            all_links = soup.find_all('a', href=True)
            notice_items = []
            for link in all_links:
                if re.search(r'202[0-9]-[0-9]{2}-[0-9]{2}', link.get_text()):
                    notice_items.append(link.parent)

        print(f"æ‰¾åˆ° {len(notice_items) if notice_items else 0} ä¸ªæ½œåœ¨é€šçŸ¥é¡¹")

        # æå–é€šçŸ¥ä¿¡æ¯
        for i, item in enumerate(notice_items):
            try:
                notice_info = extract_notice_info(item, base_url, i)
                if notice_info:
                    all_notices.append(notice_info)
                    print(f"âœ“ å·²è§£æ: {notice_info['æ ‡é¢˜']}")

                    # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)

            except Exception as e:
                print(f"âœ— è§£æç¬¬ {i + 1} ä¸ªé€šçŸ¥é¡¹æ—¶å‡ºé”™: {e}")
                continue

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é€šçŸ¥ï¼Œå°è¯•ç›´æ¥æœç´¢é¡µé¢ä¸­çš„æ–‡æœ¬
        if not all_notices:
            print("å°è¯•ç›´æ¥æœç´¢é¡µé¢æ–‡æœ¬å†…å®¹...")
            all_notices = extract_from_page_text(response.text, base_url)

        return all_notices

    except requests.exceptions.RequestException as e:
        print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return all_notices
    except Exception as e:
        print(f"è§£æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return all_notices


def extract_notice_info(item, base_url, index):
    """
    ä»å•ä¸ªé€šçŸ¥é¡¹ä¸­æå–ä¿¡æ¯
    """
    try:
        # æŸ¥æ‰¾é“¾æ¥
        link_element = item.find('a', href=True)
        if not link_element:
            return None

        # æå–æ ‡é¢˜
        title = link_element.get_text(strip=True)
        if not title or len(title) < 5:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜
            return None

        # æå–é“¾æ¥
        relative_url = link_element.get('href')
        if not relative_url:
            return None

        # è½¬æ¢ä¸ºç»å¯¹URL
        full_url = urljoin(base_url, relative_url)

        # æå–æ—¥æœŸ - å¤šç§å°è¯•
        date_text = None

        # æ–¹æ³•1ï¼šåœ¨é“¾æ¥æ–‡æœ¬ä¸­æŸ¥æ‰¾æ—¥æœŸ
        date_match = re.search(r'202[0-9]-[0-9]{2}-[0-9]{2}', link_element.get_text())
        if date_match:
            date_text = date_match.group()

        # æ–¹æ³•2ï¼šåœ¨çˆ¶å…ƒç´ ä¸­æŸ¥æ‰¾æ—¥æœŸ
        if not date_text:
            parent_text = item.get_text()
            date_match = re.search(r'202[0-9]-[0-9]{2}-[0-9]{2}', parent_text)
            if date_match:
                date_text = date_match.group()

        # æ–¹æ³•3ï¼šæŸ¥æ‰¾é™„è¿‘çš„æ—¥æœŸå…ƒç´ 
        if not date_text:
            date_element = item.find('span', class_=re.compile(r'time|date'))
            if date_element:
                date_match = re.search(r'202[0-9]-[0-9]{2}-[0-9]{2}', date_element.get_text())
                if date_match:
                    date_text = date_match.group()

        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°æ—¥æœŸï¼Œè®¾ä¸ºæœªçŸ¥
        if not date_text:
            date_text = "æœªçŸ¥æ—¥æœŸ"

        # è·å–é€šçŸ¥è¯¦æƒ…ï¼ˆå®Œæ•´å†…å®¹ï¼‰
        detail_content, content_length = get_notice_detail_full(full_url)

        return {
            'åºå·': index + 1,
            'æ ‡é¢˜': title,
            'å‘å¸ƒæ—¶é—´': date_text,
            'é“¾æ¥': full_url,
            'è¯¦æƒ…å†…å®¹': detail_content,  # å®Œæ•´å†…å®¹
            'å†…å®¹é•¿åº¦': content_length,
            'æå–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    except Exception as e:
        print(f"æå–é€šçŸ¥ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None


def get_notice_detail_full(detail_url):
    """
    è·å–é€šçŸ¥è¯¦æƒ…å®Œæ•´å†…å®¹ï¼ˆä¸æˆªæ–­ï¼‰
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        print(f"  æ­£åœ¨è·å–è¯¦æƒ…é¡µ: {detail_url}")
        response = requests.get(detail_url, headers=headers, timeout=15)
        response.encoding = 'utf-8'

        if response.status_code != 200:
            return f"è¯¦æƒ…é¡µè®¿é—®å¤±è´¥: {response.status_code}", 0

        soup = BeautifulSoup(response.text, 'html.parser')

        # å°è¯•å¤šç§å†…å®¹å®¹å™¨
        content_selectors = [
            'div.content', 'div.article-content', 'div.detail-content',
            'div.news-content', 'div#content', 'div.main-content',
            'article', 'div.entry-content', 'div.v_news_content',
            'div.ny_main', 'div.read', 'div.WordSection1',
            'div.article-body', 'div.article-text'
        ]

        content_element = None
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                print(f"  ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                break

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬çš„div
        if not content_element:
            print("  æœªæ‰¾åˆ°æ ‡å‡†å†…å®¹å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æŸ¥æ‰¾...")
            divs = soup.find_all('div')
            best_div = None
            max_text_length = 0

            for div in divs:
                # è·³è¿‡å¯¼èˆªã€é¡µè„šç­‰éå†…å®¹åŒºåŸŸ
                if any(cls in str(div.get('class', [])).lower() for cls in
                       ['nav', 'footer', 'header', 'menu', 'sidebar']):
                    continue

                text_length = len(div.get_text(strip=True))
                if text_length > max_text_length and text_length > 100:
                    max_text_length = text_length
                    best_div = div

            if best_div:
                content_element = best_div
                print(f"  æ‰¾åˆ°æœ€é•¿æ–‡æœ¬divï¼Œé•¿åº¦: {max_text_length}å­—ç¬¦")

        if content_element:
            # æ¸…ç†å†…å®¹ï¼Œç§»é™¤ä¸å¿…è¦çš„å…ƒç´ 
            for element in content_element.find_all(['script', 'style', 'iframe', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            # ç§»é™¤ç©ºçš„divå’Œpæ ‡ç­¾
            for empty_tag in content_element.find_all(['div', 'p']):
                if len(empty_tag.get_text(strip=True)) == 0:
                    empty_tag.decompose()

            # è·å–å®Œæ•´çš„æ–‡æœ¬å†…å®¹ï¼ˆä¸æˆªæ–­ï¼‰
            text_content = content_element.get_text(strip=False, separator='\n')

            # æ¸…ç†ç©ºç™½å­—ç¬¦ä½†ä¿ç•™æ®µè½ç»“æ„
            text_content = re.sub(r'[ \t]+', ' ', text_content)  # å‹ç¼©å¤šä¸ªç©ºæ ¼
            text_content = re.sub(r'\n[ \t]*\n', '\n\n', text_content)  # å‹ç¼©å¤šä¸ªç©ºè¡Œ
            text_content = text_content.strip()

            content_length = len(text_content)
            print(f"  æå–å†…å®¹é•¿åº¦: {content_length}å­—ç¬¦")

            return text_content, content_length
        else:
            print("  æœªæ‰¾åˆ°è¯¦ç»†å†…å®¹")
            return "æœªæ‰¾åˆ°è¯¦ç»†å†…å®¹", 0

    except Exception as e:
        print(f"  è·å–è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")
        return f"è·å–è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}", 0


def extract_from_page_text(html_content, base_url):
    """
    ç›´æ¥ä»é¡µé¢æ–‡æœ¬ä¸­æå–é€šçŸ¥ä¿¡æ¯
    """
    notices = []

    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾é€šçŸ¥æ¨¡å¼
        pattern = r'([^<>]{10,}?)\s*202[0-9]-[0-9]{2}-[0-9]{2}'
        matches = re.findall(pattern, html_content)

        for i, match in enumerate(matches):
            title = match.strip()
            if len(title) > 10:  # åªä¿ç•™åˆç†é•¿åº¦çš„æ ‡é¢˜
                notices.append({
                    'åºå·': i + 1,
                    'æ ‡é¢˜': title,
                    'å‘å¸ƒæ—¶é—´': 'éœ€è¿›ä¸€æ­¥è§£æ',
                    'é“¾æ¥': base_url,
                    'è¯¦æƒ…å†…å®¹': 'éœ€è¦è®¿é—®å…·ä½“é¡µé¢è·å–',
                    'å†…å®¹é•¿åº¦': 0,
                    'æå–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })

        print(f"ä»æ–‡æœ¬ä¸­æå–åˆ° {len(notices)} æ¡é€šçŸ¥")

    except Exception as e:
        print(f"æ–‡æœ¬æå–æ—¶å‡ºé”™: {e}")

    return notices


def filter_notices_by_date(notices, target_date=None):
    """
    æŒ‰æ—¥æœŸç­›é€‰é€šçŸ¥
    """
    if target_date is None:
        # é»˜è®¤è·å–å‰ä¸€å¤©çš„é€šçŸ¥
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    filtered = [notice for notice in notices if notice['å‘å¸ƒæ—¶é—´'] == target_date]
    return filtered


def save_notices_to_file(notices, filename='sicau_notices_full.json'):
    """
    ä¿å­˜é€šçŸ¥åˆ°JSONæ–‡ä»¶ï¼ˆåŒ…å«å®Œæ•´å†…å®¹ï¼‰
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(notices, f, ensure_ascii=False, indent=2)
        print(f"å®Œæ•´é€šçŸ¥å·²ä¿å­˜åˆ°: {filename}")

        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(notice.get('å†…å®¹é•¿åº¦', 0) for notice in notices)
        print(f"æ€»å†…å®¹å­—ç¬¦æ•°: {total_chars}")

    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")


def generate_full_summary_file(notices, filename='sicau_notices_full_summary.txt'):
    """
    ç”ŸæˆåŒ…å«å®Œæ•´å†…å®¹çš„æ–‡æœ¬æ‘˜è¦æ–‡ä»¶
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("å››å·å†œä¸šå¤§å­¦ç”Ÿå‘½ç§‘å­¦å­¦é™¢é€šçŸ¥å…¬å‘Šæ±‡æ€»ï¼ˆå®Œæ•´å†…å®¹ï¼‰\n")
            f.write("=" * 60 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"é€šçŸ¥æ•°é‡: {len(notices)}æ¡\n")

            total_chars = sum(notice.get('å†…å®¹é•¿åº¦', 0) for notice in notices)
            f.write(f"æ€»å†…å®¹å­—ç¬¦æ•°: {total_chars}\n")
            f.write("=" * 60 + "\n\n")

            for notice in notices:
                f.write(f"ã€ç¬¬{notice['åºå·']}æ¡é€šçŸ¥ã€‘\n")
                f.write(f"æ ‡é¢˜: {notice['æ ‡é¢˜']}\n")
                f.write(f"å‘å¸ƒæ—¶é—´: {notice['å‘å¸ƒæ—¶é—´']}\n")
                f.write(f"é“¾æ¥: {notice['é“¾æ¥']}\n")
                f.write(f"å†…å®¹é•¿åº¦: {notice.get('å†…å®¹é•¿åº¦', 0)}å­—ç¬¦\n")
                f.write(f"æå–æ—¶é—´: {notice['æå–æ—¶é—´']}\n")
                f.write("-" * 50 + "\n")

                # å†™å…¥å®Œæ•´å†…å®¹ï¼ˆä¸æˆªæ–­ï¼‰
                f.write("é€šçŸ¥å†…å®¹:\n")
                f.write(notice['è¯¦æƒ…å†…å®¹'])
                f.write("\n\n")
                f.write("=" * 80 + "\n\n")

        print(f"å®Œæ•´å†…å®¹æ‘˜è¦å·²ä¿å­˜åˆ°: {filename}")
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(filename)} å­—èŠ‚")

    except Exception as e:
        print(f"ç”Ÿæˆå®Œæ•´æ‘˜è¦æ–‡ä»¶æ—¶å‡ºé”™: {e}")


def generate_compact_summary(notices, filename='sicau_notices_compact.txt'):
    """
    ç”Ÿæˆç®€æ´ç‰ˆæ‘˜è¦ï¼ˆåªåŒ…å«æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯ï¼‰
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("å››å·å†œä¸šå¤§å­¦ç”Ÿå‘½ç§‘å­¦å­¦é™¢é€šçŸ¥å…¬å‘Šç®€æ´ç‰ˆ\n")
            f.write("=" * 50 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"é€šçŸ¥æ•°é‡: {len(notices)}æ¡\n\n")

            for notice in notices:
                f.write(f"ã€{notice['åºå·']}ã€‘{notice['æ ‡é¢˜']}\n")
                f.write(f"   æ—¶é—´: {notice['å‘å¸ƒæ—¶é—´']} | é•¿åº¦: {notice.get('å†…å®¹é•¿åº¦', 0)}å­—ç¬¦\n")
                f.write(f"   é“¾æ¥: {notice['é“¾æ¥']}\n")
                f.write("-" * 40 + "\n")

        print(f"ç®€æ´ç‰ˆæ‘˜è¦å·²ä¿å­˜åˆ°: {filename}")

    except Exception as e:
        print(f"ç”Ÿæˆç®€æ´ç‰ˆæ‘˜è¦æ—¶å‡ºé”™: {e}")


def display_notices_preview(notices, title="å››å·å†œä¸šå¤§å­¦ç”Ÿå‘½ç§‘å­¦å­¦é™¢é€šçŸ¥å…¬å‘Š"):
    """
    æ˜¾ç¤ºé€šçŸ¥é¢„è§ˆä¿¡æ¯ï¼ˆæ§åˆ¶å°æ˜¾ç¤ºæ—¶ä»ç„¶åªæ˜¾ç¤ºéƒ¨åˆ†å†…å®¹ï¼‰
    """
    print(f"\n{'=' * 80}")
    print(f"{title} (å…±{len(notices)}æ¡)")
    print(f"{'=' * 80}")

    total_chars = sum(notice.get('å†…å®¹é•¿åº¦', 0) for notice in notices)
    print(f"æ€»å†…å®¹å­—ç¬¦æ•°: {total_chars}")

    for notice in notices:
        print(f"\nã€{notice['åºå·']}ã€‘{notice['æ ‡é¢˜']}")
        print(f"   å‘å¸ƒæ—¶é—´: {notice['å‘å¸ƒæ—¶é—´']}")
        print(f"   é“¾æ¥: {notice['é“¾æ¥']}")
        print(f"   å†…å®¹é•¿åº¦: {notice.get('å†…å®¹é•¿åº¦', 0)}å­—ç¬¦")

        # æ§åˆ¶å°åªæ˜¾ç¤ºå‰200å­—ç¬¦é¢„è§ˆ
        content_preview = notice['è¯¦æƒ…å†…å®¹'][:200] + "..." if len(notice['è¯¦æƒ…å†…å®¹']) > 200 else notice['è¯¦æƒ…å†…å®¹']
        print(f"   å†…å®¹é¢„è§ˆ: {content_preview}")
        print("-" * 80)





def send_wechat_push(notices, sckey):
    """
    ä½¿ç”¨Serveré…±æ¨é€å¾®ä¿¡é€šçŸ¥
    """
    if not notices:
        return False

    # ç”Ÿæˆæ¨é€å†…å®¹
    title = f"å·å†œç”Ÿå‘½å­¦é™¢æœ€æ–°é€šçŸ¥({datetime.now().strftime('%m-%d')})"

    # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼ˆé™åˆ¶åœ¨2000å­—ç¬¦å†…ï¼‰
    content = f"ğŸ“¢ å››å·å†œä¸šå¤§å­¦ç”Ÿå‘½ç§‘å­¦å­¦é™¢\nâ° æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
    content += f"ğŸ“‹ å…±{len(notices)}æ¡æ–°é€šçŸ¥\n\n"

    # åªæ˜¾ç¤ºå‰5æ¡é€šçŸ¥ï¼Œé¿å…å†…å®¹è¿‡é•¿
    for i, notice in enumerate(notices[:5]):
        content += f"{i + 1}. {notice['æ ‡é¢˜']}\n"
        content += f"   æ—¶é—´: {notice['å‘å¸ƒæ—¶é—´']}\n"
        if i < len(notices) - 1:  # ä¸æ˜¯æœ€åä¸€æ¡å°±åŠ ç©ºè¡Œ
            content += "\n"

    if len(notices) > 5:
        content += f"\n...è¿˜æœ‰{len(notices) - 5}æ¡é€šçŸ¥ï¼Œè¯·æŸ¥çœ‹è¯¦æƒ…"

    content += f"\n\nğŸ”— æŸ¥çœ‹è¯¦æƒ…: https://smkx.sicau.edu.cn/xwjtz/tzgg.htm"

    try:
        url = f"https://sctapi.ftqq.com/{sckey}.send"
        data = {
            "title": title,
            "desp": content
        }

        response = requests.post(url, data=data)
        result = response.json()

        if result.get('code') == 0:
            print("âœ… å¾®ä¿¡æ¨é€æˆåŠŸï¼")
            return True
        else:
            print(f"âŒ å¾®ä¿¡æ¨é€å¤±è´¥: {result.get('message')}")
            return False

    except Exception as e:
        print(f"âŒ å¾®ä¿¡æ¨é€å‡ºé”™: {e}")
        return False


def main():
    """
    ä¸»å‡½æ•°
    """
    print("å¼€å§‹çˆ¬å–å››å·å†œä¸šå¤§å­¦ç”Ÿå‘½ç§‘å­¦å­¦é™¢é€šçŸ¥å…¬å‘Šï¼ˆå®Œæ•´å†…å®¹ç‰ˆï¼‰...")
    print("=" * 60)

    # è·å–æ‰€æœ‰é€šçŸ¥
    all_notices = get_sicau_notices()

    if all_notices:
        print(f"\næˆåŠŸè·å– {len(all_notices)} æ¡é€šçŸ¥")

        # æ˜¾ç¤ºé¢„è§ˆä¿¡æ¯ï¼ˆæ§åˆ¶å°ä¸æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼‰
        display_notices_preview(all_notices)

        # ç­›é€‰å‰ä¸€å¤©çš„é€šçŸ¥
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        yesterday_notices = filter_notices_by_date(all_notices, yesterday)

        if yesterday_notices:
            print(f"\næ˜¨å¤©({yesterday})å‘å¸ƒçš„æ–°é€šçŸ¥:")
            display_notices_preview(yesterday_notices, f"å‰ä¸€å¤©({yesterday})é€šçŸ¥")
        else:
            print(f"\næ˜¨å¤©({yesterday})æ²¡æœ‰å‘å¸ƒæ–°é€šçŸ¥")

        # ä¿å­˜å®Œæ•´å†…å®¹åˆ°JSONæ–‡ä»¶
        save_notices_to_file(all_notices)

        # ç”ŸæˆåŒ…å«å®Œæ•´å†…å®¹çš„æ–‡æœ¬æ–‡ä»¶
        generate_full_summary_file(all_notices)

        # ç”Ÿæˆç®€æ´ç‰ˆæ‘˜è¦
        generate_compact_summary(all_notices)

        print("\næ‰€æœ‰æ–‡ä»¶å·²ç”Ÿæˆå®Œæˆï¼")
        print("1. sicau_notices_full.json - å®Œæ•´JSONæ•°æ®")
        print("2. sicau_notices_full_summary.txt - å®Œæ•´å†…å®¹æ–‡æœ¬ç‰ˆ")
        print("3. sicau_notices_compact.txt - ç®€æ´ç‰ˆæ‘˜è¦")

    else:
        print("æœªèƒ½è·å–åˆ°ä»»ä½•é€šçŸ¥ä¿¡æ¯")
    if all_notices:
        # Serveré…±æ¨é€ï¼ˆåªéœ€è¦ä¸€ä¸ªKEYï¼‰
        sckey = "SCT301452TyhqmvrehAIcI5GoDT6yKqxTB"  # åœ¨ https://sct.ftqq.com å…è´¹è·å–
        send_wechat_push(all_notices, sckey)


if __name__ == "__main__":
    main()